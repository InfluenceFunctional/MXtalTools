machine: "cluster"  # or "cluster"
device: "cpu"  # or "cpu"
anomaly_detection: False  # slows down the code
mode: generator  #  or 'regression' or 'figures' or 'autoencoder' or 'embedding_regression' or 'sampling' WIP or 'embedding' 'mol_generation' WIP
dataset_name: 'CSD_QM9_dataset.pt' #
misc_dataset_name: 'misc_data_for_CSD_QM9_dataset.npy' #
dataset_yaml_path: '/csp.yaml' #
extra_test_set_name: null #'test_blind_test_dataset.pkl'  #'acridin_dataset.pkl' #'test_blind_test_dataset.pkl'
save_checkpoints: True # will do it always on cluster, only locally if True
checkpointing_loss_type: test  # will save a new checkpoint when a minimum of this is reached
model_names: ['generator', 'autoencoder']
sweep_id: null
sweep_path: null

dataset:
  regression_target: crystal_packing_coefficient

# batching & convergence
min_batch_size: 100
num_samples: 1000000
max_epoch_steps: 1000
early_epochs_step_override: 5
num_early_epochs: 0
grow_batch_size: True
max_batch_size: 20
batch_growth_increment: 0.25 # fraction of batch size to grow by each epoch
overfit_tolerance: 4  # maximum allowed ratio of test_loss/train_loss
minimum_epochs: 10000
max_epochs: 0 # 0 epochs takes us straight to sampling/evaluation (WIP)
history: 50 # for convergence checks
gradient_norm_clip: 1
extra_test_period: 5 # # MUST BE MULTIPLE OF SAMPLE REPORTING FREQUENCY how often to evaluate on extra test sets (hardcoded analysis per extra test set)

model_paths:
  discriminator: null
  generator: null #cluster/best_generator_experiments_generator_tests_run4_dev_1_26-08-17-14-40
  regressor: null #'volume_model.pt'
  autoencoder: best_autoencoder_experiments_autoencoder_tests_new_qm9_13-08-13-38-49  # excellent proton model
  embedding_regressor: null
  polymorph_classifier: null # 'best_polymorph_classifier_experiments_dev_25-06-17-41-37'

logger:
  run_name: csp_dev
  experiment_tag: csp_dev
  log_figures: True

generate_sgs: ['P21/c']
supercell_size: 5

# for GAN training
generator:
  samples_per_iter: 1
  prior_loss_coefficient: 1  # initial value for the prior loss coefficient
  prior_coefficient_threshold: 0.05 #0.001
  vdw_loss_coefficient: 1
  vdw_turnover_potential: 10 # maximum value for LJ regularization
  variation_scale: 1  # maximum of the uniform distribution of search scales
  canonical_conformer_orientation: 'random' # 'standardized' 'random'

  #generator optimizer and model
  optimizer:
    beta1: 0.9
    beta2: 0.999
    convergence_eps: 1.0e-04
    init_lr: 1.0e-5
    lr_growth_lambda: 1.05
    lr_schedule: true
    lr_shrink_lambda: 0.99
    max_lr: 1.0e-3
    min_lr: 1.0e-06
    optimizer: adamw
    use_plateau_scheduler: false
    weight_decay: 0

  model:
    hidden_dim: 1024
    dropout: 0
    norm: null
    num_layers: 4
    vector_norm: null #'vector layer'


autoencoder:
  infer_protons: False
  filter_protons: False

discriminator:
  # settings
  train_adversarially: False
  train_on_randn: False
  train_on_distorted: False
  use_classification_loss: True
  use_rdf_distance_loss: True
  distortion_magnitude: -1 # -1 for wide range test # noise for distorted generation

  # discriminator optimizer and model
  optimizer:
    optimizer: adamw
    init_lr: 1.0E-5
    max_lr: 1.0E-6
    min_lr: 5.0E-6
    lr_schedule: True
    beta1: 0.9
    beta2: 0.999
    weight_decay: 0.1
    convergence_eps: 0.0001
    lr_growth_lambda: 1.00
    lr_shrink_lambda: 0.95
    use_plateau_scheduler: False

  model:
    graph_aggregator: 'softmax'
    periodic_convolution_type: 'all_layers'  # or 'last_layer'
    activation: 'gelu'

    graph:
      node_dim: 16
      message_dim: 16
      embedding_dim: 16
      num_convs: 1
      fcs_per_gc: 1
      num_radial: 32
      num_input_classes: 101
      norm: null
      dropout: 0

      atom_type_embedding_dim: 32
      radial_embedding: 'bessel'
      cutoff: 2
      max_num_neighbors: 100
      envelope_exponent: 5

    fc:
      hidden_dim: 16
      num_layers: 1
      dropout: 0
      norm: null
