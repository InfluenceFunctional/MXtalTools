machine: "local"  # or "cluster"
device: "cuda"  # or "cpu"
anomaly_detection: False  # slows down the code
mode: proxy_discriminator
dataset_name: test_pd_dataset.pt #'test_dataset.pt'
dataset_yaml_path: '/proxy_discriminator.yaml'
save_checkpoints: True # will do it always on cluster, only locally if True
checkpointing_loss_type: test  # will save a new checkpoint when a minimum of this is reached
model_names: ['autoencoder', 'proxy_discriminator']
sweep_id: null
sweep_path: null

dataset:
  type: 'molecule'
  max_dataset_length: 10000000
  test_fraction: 0.2
  filter_protons: False
  regression_target: null
  single_identifier: null #100558
  buffer_size: 100000
  otf:
    smiles_source: 'D:\crystal_datasets\zinc22'#'/home/mkilgour/crystal_datasets/zinc22/'  # /scratch/mk8347/zinc22
    build_size: 200
    max_num_atoms: 30
    pare_to_size: 9
    max_num_heavy_atoms: 9
    space_group: 1
    max_radius: 15
    post_scramble_each: 10
    processes: 4
    allowed_atom_types: [ 1, 6, 7, 8, 9 ]
  loader_processes: 0

# batching & convergence
early_epochs_step_override: 5
num_early_epochs: 0
grow_batch_size: True
min_batch_size: 50
max_batch_size: 2000
batch_growth_increment: 0.1 # fraction of batch size to grow by each epoch
overfit_tolerance: 4  # maximum allowed ratio of test_loss/train_loss
minimum_epochs: 50
max_epochs: 10000 # 0 epochs takes us straight to sampling/evaluation (only implemented for GAN)
history: 50  # for convergence checks
gradient_norm_clip: 1
extra_test_period: 1 # # MUST BE MULTIPLE OF SAMPLE REPORTING FREQUENCY how often to evaluate on extra test sets (hardcoded analysis per extra test set)

logger:
  run_name: dev
  experiment_tag: dev
  mini_csp_frequency: 5  # how often to run CSP-style search WIP
  sample_reporting_frequency: 5  # how often to do detailed reporting with figures
  log_figures: True

sample_steps: 100

seeds:
  model: 12345
  dataset: 0

# for reloading prior checkpoints
model_paths:
  discriminator: null
  generator: null
  regressor: null
  autoencoder: null
  embedding_regressor: null
  proxy_discriminator: null

positional_noise:
  discriminator: 0
  generator: 0
  regressor: 0
  autoencoder: 0
  embedding_regressor: 0
  proxy_discriminator: 0

generate_sgs: [1]
supercell_size: 5

proxy_discriminator:
  embedding_type: autoencoder # autoencoder, principal_axes, principal_moments, mol_volume
  train_encoder: False  # deprecated right now # option to train the encoder, if doing autoencoder training
  electrostatic_scaling_factor: 100  # training energy is scaled_lj + factor * es_energy
  train_on_mace: True  # alternately, train on the mace lattice energy
  cutoff: 6
  optimizer:
    beta1: 0.9
    beta2: 0.999
    convergence_eps: 1.0e-04
    init_lr: 1.0e-4
    lr_growth_lambda: 1.1
    lr_schedule: true
    lr_shrink_lambda: 0.975
    max_lr: 1.0e-03
    min_lr: 5.0e-06
    optimizer: adamw
    use_plateau_scheduler: false
    weight_decay: 0.01
    overwrite_on_reload: False

  model:
    hidden_dim: 1024
    dropout: 0
    norm: null
    num_layers: 4
    vector_norm: null
